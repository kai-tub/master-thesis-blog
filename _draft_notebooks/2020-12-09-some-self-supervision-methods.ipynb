{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some self-supervision methods\n",
    "> An overview of some self-supervision methods.\n",
    "\n",
    "- toc: true\n",
    "- categories: self-supervision\n",
    "- comments: true\n",
    "- badges: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "In the previous post, we got a short introduction to self-supervised learning methods.\n",
    "The main components can be seen in <a href=\"#Fig1\">Fig. 1</a> and\n",
    "the different types of pretext tasks in <a href=\"#Fig2\">Fig. 2</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"Fig1\">\n",
    "        <div>\n",
    "            <figure>\n",
    "<img src=\"2020-11-25/self-supervised-learning.svg\" alt=\"Flowchart of a self-supervised learning loop\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig. 1: Self-supervised training loop</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"Fig2\">\n",
    "        <div>\n",
    "            <figure>\n",
    "<img src=\"2020-11-25/pretext-tasks.svg\" alt=\"Mindmap of the three big pretext task types\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig. 2: The three pretext task types</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basic understanding of self-supervised learning methods, we will review popular variants and think about their disadvantages in the remote sensing domain. The ... post gives us a quick comparison between _classic_ and _remote sensing_ images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RotNet\n",
    "\n",
    "As an example, we already introduced _RotNet_ {% cite RotNet %}.\n",
    "The pretext task of RotNet is to recognize if the image was rotated by 0, 90, 180, or 270 degrees. \n",
    "The rotation values themselves are the pseudolabels.\n",
    "As the pseudolabels are generated from the images, the learning method falls into the _context-based_ class.\n",
    "The core intuition is that the correct rotation can only be chosen, when one understands the depicted objects.\n",
    "The simple method was able to get state-of-the-art results in 2018!\n",
    "Some beneficial side effect of the simplicity are:\n",
    "- No low-level visual artifacts\n",
    "    - Rotating the image by a multiple of 90 degrees does not require any resizing/interpolation\n",
    "- Pseudolabel generation is fast\n",
    "- No special pre-processing routine in order to avoid learning trivial visual features{% fn 1 %}\n",
    "- Well-posedness\n",
    "    - The \"classic\" images have a correct orientation/a top and bottom\n",
    "    - There is, usually, no ambiguity by how much an object was rotated\n",
    "    \n",
    "### RS issues\n",
    "In the domain of remote sensing the core assumption of RotNet does not hold.\n",
    "For aerial imagery the objects are rotation invariant, i.e. there is no _up or down_. As a result the well-posedness is violated.\n",
    "The method should not work in the RS domain, as the pretext task is ill-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepCluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "{% bibliography --cited_in_order %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{ 'We will see when this is necessary in the following example.' | fndetail: 1 }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My ideas\n",
    "My idea is to utilize the rotation invariance of the data.\n",
    "RotNet is based on the rotation _variance_. In other words, it is based on the fact that the objects/scenes had an *up* and *down*. The rotation is the label that should be predicted by the model. To solve this pretext-task the model should learn to identify the objects themselves, otherwise it wouldn't know the correct orientation.\n",
    "\n",
    "Due to the invariance of satellite data, this approach should not work.\n",
    "But what if we exploit the invariance of satellite data?\n",
    "If we rotate the image, the same/similar feature vector should be obtained.\n",
    "So what if use a siamese network with two input images and let the network try to identify the rotated original image?\n",
    "Or something similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myfastai] *",
   "language": "python",
   "name": "conda-env-myfastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
