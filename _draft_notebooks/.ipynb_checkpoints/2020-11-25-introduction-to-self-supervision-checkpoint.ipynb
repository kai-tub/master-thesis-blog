{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to self-supervised visual learning\n",
    "> A short introduction to self-supervised visual-based learning methods.\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: self-supervised-learning\n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import graphviz\n",
    "import pathlib\n",
    "import shutil\n",
    "from IPython.display import SVG\n",
    "\n",
    "asset_path = pathlib.Path(\"./2020-11-25\")\n",
    "asset_path.mkdir(exist_ok=True)\n",
    "\n",
    "def generate_graph(\n",
    "    inp, \n",
    "    fname=\"tmp\", \n",
    "    path=asset_path, \n",
    "    engine=\"dot\",\n",
    "    mode=\"LR\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates and renders a DOT graph as an SVG file.\n",
    "    Return the location of the saved file.\n",
    "    \"\"\"\n",
    "    # WSL error if run in WSL Path\n",
    "    # First create in home and then move file\n",
    "    tmp_path = pathlib.Path(\"~\").expanduser()\n",
    "    graph = graphviz.Source(\n",
    "        source=f'digraph G{{ rankdir=\"{mode}\" {inp} ;}}',\n",
    "        filename=fname,\n",
    "        format=\"svg\",\n",
    "        directory=tmp_path,\n",
    "        engine=engine\n",
    "    )\n",
    "    loc = graph.render()\n",
    "    target_loc = asset_path / f\"{fname}.svg\"\n",
    "    shutil.move(loc, target_loc)\n",
    "    return target_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "After understanding the remote sensing data we will be working with, we will focus on the deep-learning part of my master thesis, specifically self-supervised learning.\n",
    "For the following deep-learning topics, it would be helpful to have a general understanding of what deep-learning, or more generally, machine learning is. Still, I will try my best to keep the content comfortable enough for interested readers to follow along. :relaxed:\n",
    "\n",
    "There are many great introduction resources and courses for machine learning! For a quick introduction to deep-learning, I highly recommend [3Blue1Brown's](https://www.youtube.com/c/3blue1brown) series on neural networks:\n",
    "\n",
    "> youtube: https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "<center>3Blue1Brown's tutorial on neural networks</center>\n",
    "\n",
    "A different highly-compressed resource for machine learning in general is [The Hundred-Page Machine Learning Book](https://leanpub.com/theMLbook) by Andriy Burkov.\n",
    "\n",
    "With that out of the way, let's take a quick refresher on learning methods and see what *self-supervised* learning methods are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Methods\n",
    "\n",
    "In general, there are three primary methods to train deep neural networks:\n",
    "- Supervised Learning\n",
    "- Unsupervised Learning\n",
    "- Reinforcement Learning\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "Most current state-of-the-art computer vision applications utilize  *supervised learning methods*. In this setting, the input data are *labeled* images. Humans annotate these labels. A simple scenario would be a dataset for a classification application with the two classes, \"dog\" and \"cat\". Here, a human would look at \n",
    "<a href=\"#Fig1\">Fig. 1</a>\n",
    "and conclude that it should belong to the category \"dog\" and annotate it as such.\n",
    "\n",
    "<figure>\n",
    "        <div>\n",
    "            <figure id=\"Fig1\">\n",
    "<img src=\"2020-11-25/puppy.jpg\" alt=\"Image of a puppy\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig 1: Example image of a dog (Image by <a href=\"https://pixabay.com/users/3194556-3194556/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1903313\">Karen Warfel</a> from <a href=\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1903313\">Pixabay</a>)</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "This process is repeated for every image in the dataset until all images are labeled. Then the learning process starts. The model tries to predict the correct label, and *learns* from its mistake by comparing its predictions to the *ground-truth* labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "generate_graph(\n",
    "    \"\"\"\n",
    "    ordering=in\n",
    "    nodesep=0.60\n",
    "    fontsize=12\n",
    "\n",
    "    Model[shape=box style=rounded width=1 height=0.7 label=Model]\n",
    "    Data -> Model -> Prediction [weight=10]; \"Human Labels\" -> Loss;\n",
    "    Prediction -> Loss; \n",
    "    Loss -> Model [label=\"Update\" tailport=s headport=s]\"\"\",\n",
    "    fname=\"supervised-learning\"\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"Fig2\">\n",
    "<img class=\"docimage\" src=\"2020-11-25/supervised-learning.svg\" alt=\"Flowchart of supervised training\">\n",
    "    <figcaption><center>Fig. 2: Supervised training loop</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#Fig2\">Fig. 2</a> shows the general learning, or training, loop of a supervised method.\n",
    "1. Data is fed into the Model. For example, many images of dogs and cats.\n",
    "2. The model *predicts* an output. In our scenario, it could guess that the current image is a dog.\n",
    "3. The prediction of the model is compared to the *correct* label. The difference between the correct label and the prediction is mathematically expressed as a loss.\n",
    "4. The loss is then used to tell the model how *wrong* the prediction was and is used to update the model's parameters. This update procedure is the *learning* part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although supervised-learning leads to the highest model performance, there is one significant limitation: cost. The cost of annotating all images is immense. The standard large-scale image dataset used to compare research results, ImageNet {% cite Russakovsky2015 %}, has more than 1.3 million labeled images with 1,000 classes! These images have to be manually annotated and verified. The necessary steps to gather data for supervised-learning can be seen in <a href=\"#Fig3\">Fig. 3</a>. The annotation process is even worse for video datasets, as these have a temporal dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('2020-11-25/data-gathering-label.svg')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "generate_graph(\n",
    "    \"\"\"\n",
    "    Labeling[color=\"red\"]\n",
    "    \"Data acquisition\" -> Verifying;\n",
    "    \"Data acquisition\" -> Cleaning;\n",
    "    \"Data acquisition\" -> Preprocessing;\n",
    "    Verifying -> Labeling;\n",
    "    Cleaning -> Labeling;\n",
    "    Preprocessing -> Labeling;\n",
    "    Labeling -> \"Re-Verify\"\n",
    "    \"\"\",\n",
    "    fname=\"data-gathering-label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"Fig3\">\n",
    "<img class=\"docimage\" src=\"2020-11-25/data-gathering-label.svg\" alt=\"Flowchart of the data gathering process for supervised-learning methods\">\n",
    "    <figcaption><center>Fig. 3: Data gathering process for supervised-learning methods</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "An alternative is unsupervised learning, which does not reach the same model performance as supervised learning (yet).\n",
    "But, unsupervised learning has one significant practical advantage..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "\n",
    "Unsupervised learning methods do not require *any* human-annotated labels. The specific methods use different approaches to *learn* from the data itself. They iterate and update their predictions in various ways and usually define their loss based on their previous predictions. The process of iterating through the data and *learning* from some definition of error/loss remains. Compare <a href=\"#Fig2\">Fig. 2</a> with <a href=\"#Fig4\">Fig. 4</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "generate_graph(\n",
    "    \"\"\"\n",
    "    ordering=in\n",
    "    nodesep=0.60\n",
    "    fontsize=12\n",
    "\n",
    "    Model[shape=box style=rounded width=1 height=0.7 label=Model]\n",
    "    Data -> Model -> Prediction [weight=10]; \n",
    "    Prediction -> Loss [weight=10]; \n",
    "    Loss -> Model [label=\"Update\" tailport=s headport=s]\"\"\",\n",
    "    fname=\"unsupervised-learning\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"Fig4\">\n",
    "        <div>\n",
    "            <figure>\n",
    "<img src=\"2020-11-25/unsupervised-learning.svg\" alt=\"Flowchart of unsupervised learning loop\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig. 4: Unsupervised training loop</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "A classic unsupervised machine learning method is [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering). Here the data is grouped into clusters of data points that are close to each other. What these clusters *represent* is not essential for the algorithm. The interpretation is left for human evaluation. <a href=\"#Fig5\">Fig. 5</a> shows the result of k-means on two-dimensional data. As we can see, no information about the data points themselves is required. The algorithm simply uses the *distance* between the points to *group* them. The main idea being that these groups have something *interesting* in common and belong together.\n",
    "\n",
    "\n",
    "<figure id=\"Fig5\">\n",
    "        <div>\n",
    "            <figure>\n",
    "<img src=\"2020-11-25/k-means.png\" alt=\"Image of a k-means result\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig. 5: Example solution of k-means (Image by Akshay Singhal from <a href=\"https://www.gatevidyalay.com/k-means-clustering-algorithm-example/\">Gatevidyalay</a>)</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "In the context of deep-learning, the time spent to train unsupervised models is generally similar to the training time of supervised methods. No time is saved during the training process! \n",
    "The main reason why unsupervised-learning methods are so interesting is because they don't require any labels!\n",
    "The absence of labels reduces the overall cost and time to generate the required data and *may* lead to faster production times.\n",
    "\n",
    "One promising subset of unsupervised learning methods for deep learning is called *self-supervised* learning.\n",
    "We will take a closer look at self-supervised learning in the next section.\n",
    "Before that, let's look at the last significant learning approach:\n",
    "reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning\n",
    "Reinforcement learning is somewhat very different from the previous\n",
    "learning methods due to the learning procedure's nature. Reinforcement learning does not learn from *static* data but from interactions with its *environment* as an *actor*, see <a href=\"#Fig6\">Fig. 6</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "generate_graph(\n",
    "    \"\"\"\n",
    "    ordering=in\n",
    "    nodesep=0.60\n",
    "    fontsize=12\n",
    "\n",
    "     Agent[shape=box style=rounded width=1 height=0.7 label=Agent]\n",
    "     Agent -> Environment [label=\"interacts\" headport=n tailport=n weight=0]; \n",
    "     Environment -> Agent [label=\"Reward & State update\" headport=s tailport=s weight=0]\n",
    "     \"\"\",\n",
    "    fname=\"reinforcement-learning\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"Fig6\">\n",
    "        <div>\n",
    "            <figure>\n",
    "<img src=\"2020-11-25/reinforcement-learning.svg\" alt=\"Flowchart of reinforcement learning loop\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig. 6: Reinforcement training loop</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, the model tries to learn the actions which will get the highest reward. Classical variations use reinforcement learning to beat computer or board games. One model with high media coverage was [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo). AlphaGo beat the 18-time world champion Lee Sedol in the board game Go,\n",
    "a too hard game for machines to master for a very long time. Because reinforcement learning is so very different, we won't be going into any more detail, even if it is a fascinating subject. \n",
    "\n",
    "Let's go back to the previous, shortly introduced unsupervised learning method, *self-supervised* learning, as this method will be sticking with us for a very long time.\n",
    "\n",
    "##  Self-supervised learning\n",
    "Self-supervised learning was introduced as an unsupervised learning method. Therefore, self-supervised learning methods do not require any human-annotated labels. At this point, you might ask why the term *human-annotated* was always mentioned in conjunction with labels instead of just labels. The reason is that self-supervised methods create their labels! \n",
    "\n",
    "Instead of relying on labels from hard-working humans, these methods automatically generate their own labels, *pseudolabels*, through a so-called *pretext task*. A general self-supervised training loop can be seen in <a href=\"#Fig7\"> Fig. 7</a>.\n",
    "These pretext tasks are very different from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "generate_graph(\n",
    "    \"\"\"\n",
    "    ordering=in\n",
    "    nodesep=0.60\n",
    "    fontsize=12\n",
    "\n",
    "    Model[shape=box style=rounded width=1 height=0.7 label=Model]\n",
    "    Data -> Model -> Prediction [weight=10]; \n",
    "    \"Pretext\\ntask\" -> \"Pseudolabels\" [weight=10];\n",
    "    \"Pseudolabels\" -> Loss;\n",
    "    Prediction -> Loss; \n",
    "    Loss -> Model [label=\"Update\" tailport=s headport=s]\"\"\",\n",
    "    fname=\"self-supervised-learning\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"Fig7\">\n",
    "        <div>\n",
    "            <figure>\n",
    "<img src=\"2020-11-25/self-supervised-learning.svg\" alt=\"Flowchart of a self-supervised learning loop\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig. 7: Self-supervised training loop</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, an ingenious idea was to, first, simply rotate the images by\n",
    "either 0°, 90°, or 270°. Then, to let the network guess by how much the image was rotated. {% cite Gidaris2018 %}\n",
    "The idea is that the model learns\n",
    "the objects' visual features to recognize the _correct_ orientation of the image. In the end, this is nothing more than a classification task, where the pseudolabels are generated by randomly rotating the image. See <a href=\"#Fig8\">Fig. 8</a> for an overview. The pretext task isn't\n",
    "limited to creating pseudolabels from the input data. Some variations\n",
    "combine the input with the model's prediction to create\n",
    "pseudolabels, and others combine multiple pretext tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"340pt\" height=\"418pt\" viewBox=\"0.00 0.00 340.47 418.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 414)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-414 336.474,-414 336.474,4 -4,4\"/>\n",
       "<!-- Model -->\n",
       "<g id=\"node1\" class=\"node\"><title>Model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M156.68,-194C156.68,-194 108.68,-194 108.68,-194 102.68,-194 96.6802,-188 96.6802,-182 96.6802,-182 96.6802,-156 96.6802,-156 96.6802,-150 102.68,-144 108.68,-144 108.68,-144 156.68,-144 156.68,-144 162.68,-144 168.68,-150 168.68,-156 168.68,-156 168.68,-182 168.68,-182 168.68,-188 162.68,-194 156.68,-194\"/>\n",
       "<text text-anchor=\"middle\" x=\"132.68\" y=\"-165.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Model</text>\n",
       "</g>\n",
       "<!-- Prediction -->\n",
       "<g id=\"node5\" class=\"node\"><title>Prediction</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"132.68\" cy=\"-90\" rx=\"48.1917\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"132.68\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Prediction</text>\n",
       "</g>\n",
       "<!-- Model&#45;&gt;Prediction -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>Model-&gt;Prediction</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M132.68,-143.637C132.68,-135.672 132.68,-126.769 132.68,-118.584\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.18,-118.376 132.68,-108.376 129.18,-118.376 136.18,-118.376\"/>\n",
       "</g>\n",
       "<!-- Image -->\n",
       "<g id=\"node2\" class=\"node\"><title>Image</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"132.68\" cy=\"-392\" rx=\"32.4942\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"132.68\" y=\"-388.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Image</text>\n",
       "</g>\n",
       "<!-- Randomly rotate -->\n",
       "<g id=\"node3\" class=\"node\"><title>Randomly rotate</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"132.68\" cy=\"-320\" rx=\"71.4873\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"132.68\" y=\"-316.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Randomly rotate</text>\n",
       "</g>\n",
       "<!-- Image&#45;&gt;Randomly rotate -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>Image-&gt;Randomly rotate</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M132.68,-373.697C132.68,-365.983 132.68,-356.712 132.68,-348.112\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.18,-348.104 132.68,-338.104 129.18,-348.104 136.18,-348.104\"/>\n",
       "</g>\n",
       "<!-- Rotated Image -->\n",
       "<g id=\"node4\" class=\"node\"><title>Rotated Image</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"132.68\" cy=\"-248\" rx=\"63.8893\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"132.68\" y=\"-244.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Rotated Image</text>\n",
       "</g>\n",
       "<!-- Randomly rotate&#45;&gt;Rotated Image -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>Randomly rotate-&gt;Rotated Image</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M132.68,-301.697C132.68,-293.983 132.68,-284.712 132.68,-276.112\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.18,-276.104 132.68,-266.104 129.18,-276.104 136.18,-276.104\"/>\n",
       "</g>\n",
       "<!-- Pseudolabel=90° -->\n",
       "<g id=\"node7\" class=\"node\"><title>Pseudolabel=90°</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"259.68\" cy=\"-169\" rx=\"72.5877\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"259.68\" y=\"-165.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Pseudolabel=90°</text>\n",
       "</g>\n",
       "<!-- Randomly rotate&#45;&gt;Pseudolabel=90° -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>Randomly rotate-&gt;Pseudolabel=90°:n</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.4,-303.721C199.666,-284.412 252.551,-247.624 259.023,-198.078\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"262.522,-198.206 259.68,-188 255.537,-197.751 262.522,-198.206\"/>\n",
       "</g>\n",
       "<!-- Rotated Image&#45;&gt;Model -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>Rotated Image-&gt;Model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M132.68,-229.911C132.68,-222.332 132.68,-213.13 132.68,-204.235\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.18,-204.05 132.68,-194.05 129.18,-204.05 136.18,-204.05\"/>\n",
       "</g>\n",
       "<!-- Loss -->\n",
       "<g id=\"node6\" class=\"node\"><title>Loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"132.68\" cy=\"-18\" rx=\"28.6953\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"132.68\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Loss</text>\n",
       "</g>\n",
       "<!-- Prediction&#45;&gt;Loss -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>Prediction-&gt;Loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M132.68,-71.6966C132.68,-63.9827 132.68,-54.7125 132.68,-46.1124\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.18,-46.1043 132.68,-36.1043 129.18,-46.1044 136.18,-46.1043\"/>\n",
       "</g>\n",
       "<!-- Loss&#45;&gt;Model -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>Loss:w-&gt;Model:w</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M102.68,-18C39.0398,-18 29.4026,-153.494 85.6684,-167.785\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.3312,-171.269 95.6802,-169 86.1749,-164.32 85.3312,-171.269\"/>\n",
       "<text text-anchor=\"middle\" x=\"24.5\" y=\"-93.8432\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Update  </text>\n",
       "</g>\n",
       "<!-- Pseudolabel=90°&#45;&gt;Loss -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>Pseudolabel=90°:s-&gt;Loss:e</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M259.68,-150C259.68,-80.7514 237.789,-23.8134 173,-18.4181\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"172.814,-14.9078 162.68,-18 172.53,-21.902 172.814,-14.9078\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "SVG(generate_graph(\"\"\"\n",
    "    Model[shape=box style=rounded width=1 height=0.7 label=Model]\n",
    "    \n",
    "    Image -> \"Randomly rotate\";\n",
    "    \"Randomly rotate\" -> \"Rotated Image\";\n",
    "    \"Rotated Image\" -> Model[weight=10];\n",
    "    Model -> Prediction[weight=8];\n",
    "    Prediction-> Loss[weight=8];\n",
    "    \"Randomly rotate\" -> \"Pseudolabel=90°\"[weight=0, headport=n];\n",
    "    \"Pseudolabel=90°\" -> Loss[weight=0 headport=e tailport=s];\n",
    "    Loss -> Model[xlabel=\"Update  \", tailport=w, headport=w]\n",
    "    \"\"\",\n",
    "    mode=\"tb\",\n",
    "    fname=\"rot-chart\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"Fig8\">\n",
    "        <div>\n",
    "            <figure>\n",
    "<img src=\"2020-11-25/rot-chart.svg\" alt=\"Flowchart of rotation based self-supervised learning\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig. 8: Rotation based self-supervised learning</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what is the result of such an unsupervised training loop? In short, a *trained* model. The model should detect and differentiate objects from each other without being explicitly taught what these objects are. \n",
    "Some applications then work with the trained model without changing it.\n",
    "Others *finetune* the model on a similar, small labeled dataset. In the finetune procedure, only minor changes to the model are made. Here, we are trying to tell the model what objects we are *interested* in.\n",
    "\n",
    "Looking back to our previous example, we could train a model on random images from [Flickr](https://www.flickr.com/explore) without labeling them. Afterwards, we finetune our model to differentiate between dogs and cats by supplying a small labeled dataset. The model should perform very well without a lot of training because it already *knows* how to differentiate dogs and cats by looking at random images from Flickr. At least, this is what we hope. Our dataset *pushes* the model to focus on a specific task.\n",
    "\n",
    "In the research community, the self-supervised learning methods are evaluated by a similar procedure.\n",
    "First, the model is trained on a dataset without using any human-annotated labels and is then finetuned on a *downstream task* of a different dataset. A possible downstream task would be to classify dogs and cats. The score of the downstream tasks is used as a quantitive measure of the generability of the model.\n",
    "\n",
    "<!-- How are these models being deployed? -->\n",
    "\n",
    "The authors from Jing _et. al_ {% cite Jing2020 %} gave a very detailed overview of the various self-supervision approaches. For images, there are three significant types of pretext tasks, as shown in <a href=\"#Fig9\">Fig. 9</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "generate_graph(\n",
    "    \"\"\"\n",
    "    \"Pretext tasks\" -> \"Generation-based\\nmethods\"\n",
    "    \"Pretext tasks\" -> \"Context-based methods\"\n",
    "    \"Pretext tasks\" -> \"Free Semantic\\nLabel-based methods\"\n",
    "    \"\"\",\n",
    "    mode=\"TB\",\n",
    "    fname=\"pretext-tasks\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"Fig9\">\n",
    "        <div>\n",
    "            <figure>\n",
    "<img src=\"2020-11-25/pretext-tasks.svg\" alt=\"Mindmap of the three big pretext task types\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig. 9: The three pretext task types</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *generation-based* methods, the model has to *generate* data to match the pseudolabel. For example, the model gets a grayscale image and has to colorize the image. The pseudolabel would be the original RGB image by itself! A different generation based method would be to *cut* out parts of the image and to let the model try to synthesize the missing region.\n",
    "\n",
    "Our previous rotation example, where the model had to guess the image's rotation, is a _context-based_ method. The context can be derived from the image itself (rotation, solving spatial puzzles) or by context similarity.\n",
    "\n",
    "Both of these pretext tasks implicitly force the networks to learn semantic features. In our rotation example, we never tell the model what\n",
    "the objects are, but it has to learn various objects' features to predict the rotation correctly. If one doesn't know how animals, walls, water, ground, sky, hills, etc. look like, you couldn't predict how much an image was rotated.\n",
    "\n",
    "*Free semantic label-based* methods take a more direct approach. Here the manual annotation effort is bypassed by automatically generating *true* labels. Hence the name, *free semantic label*. Usually, game engines are utilized to generate object maps. In other words, we generate the images with the help of the game engine, and because we know what we are rendering and where we are putting the objects, we get the *labels* for free.  Due to the synthetic nature, the pixels that belong to an object can be automatically generated, or the object names themselves can be exported.\n",
    "Here, the model directly learns visual features; *But* there is a domain gap between synthetic and natural images. \n",
    "\n",
    "Each method has its strength and weakness. But, before we go into any more detail, let's summarize our findings so far. \n",
    "\n",
    "## Summary\n",
    "\n",
    "These are the main things you should take away from this post:\n",
    "1. Currently, supervised learning methods lead to the best performance, but require human-annotated images\n",
    "1. Annotating images is costly and takes a lot of time, good unsupervised approaches could open up deep-learning for even more use-cases.\n",
    "1. Self-supervised learning is a subset of unsupervised learning.\n",
    "1. As a subset of unsupervised learning, self-supervised learning does not require any labels.\n",
    "1. Self-supervised methods generate their *own* labels, pseudolabels, by completing a pretext task\n",
    "1. There are three different pretext tasks\n",
    "    - generation-based methods\n",
    "    - context-based methods\n",
    "    - free semantic labels-based methods\n",
    "\n",
    "**PS:**\n",
    "\n",
    ":tada: You made it until the end of this long post! :tada:\n",
    "\n",
    "This post was quite long and is probably hard to follow without any prior knowledge of machine/deep-learning. Hopefully, the main points of the posts were still clear. \n",
    "\n",
    "In the next posts, we will take a closer look at the different self-supervision based methods and think about how applicable they would be in a remote sensing setting.\n",
    "\n",
    "Until then, have a productive time! :+1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "{% bibliography --cited_in_order %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myfastai] *",
   "language": "python",
   "name": "conda-env-myfastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
