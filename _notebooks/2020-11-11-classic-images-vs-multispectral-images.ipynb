{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic vs remote sensing multispectral images\n",
    "> A comprehensive comparison between classic RGB images and remote sensing multispectral images. \n",
    "\n",
    "- toc: true\n",
    "- hide: true\n",
    "- search_exclude: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "In this post we are going to do a detailed comparison between classic and multispectral images.\n",
    "\n",
    "TODO: Write something in more detail\n",
    "\n",
    "## Values\n",
    "The [introduction to mulit-channel images post](https://kai-tub.github.io/master-thesis-blog/images/2020/09/16/images-with-channels.html) explained in great detail how the RGB values are stored on disk. In short, colored images generally use three channels: red (R), green (G), and blue (B). The channels have a predefined _range_. The range, or the number of distinct colors, is called the color-depth. In the deep-learning field these RGB images usually have a color-depth of 8-bit, resulting in 256 (=2⁸) color tones per channel and ~16 million (=2⁸ x 2⁸ x 2⁸) different color values in total. \n",
    "\n",
    "In contrast to RGB images with three channels, we saw multispectral remote sensing data with double-digit channels.\n",
    "The exact amount of channels heavily varies between measuring equipment but for now we will focus on\n",
    "images from the Sentinel-2 satellites with 13 bands (TODO: Add link).\n",
    "The satellites sense electromagnetic waves ranging from the visible/near-infrared spectrum to the shortwave-infrared band.\n",
    "The main idea being that the information outside of the visible spectrum helps to better differentiate between objects, even if they *look* the same in the visible light. For multispectral images the number of bits per channel is not called color-depth, instead it is referred to as radiometric resolution. \n",
    "The radiometric resolution for Sentinel-2 images effectively is 16-bits per band, resulting in much larger image files.\n",
    "\n",
    "\n",
    "<a></a>\n",
    "<center>Comparing values of classic and Sentinel-2 images</center>\n",
    "\n",
    "|                                      | Classic RGB images | Sentinel-2 multispectral images |\n",
    "|--------------------------------------|:------------------:|:-------------------------------:|\n",
    "| # Channels / Bands                   |          3         |                13               |\n",
    "| Color-depth / Radiometric resolution |        8-bit       |              16-bit             |\n",
    "| Size of 224px x 224px image          |       147 KiB      |            1,274 KiB            |\n",
    "| Relative size to classic RGB image   |          1         |               8.67              |\n",
    "\n",
    "\n",
    "\n",
    "## Image content\n",
    "Although it is important to know how the data is digitally represented, we mostly care about the images themselves and not the binary values. \n",
    "Most visual deep-learning applications use pretrained models tuned on the popular image dataset ImageNet {% cite Russakovsky2015 %}. ImageNet consists of about 1.3 million images from animals, vehicles, tools, and many more everyday objects. Most of them are photos where the _interesting_ object lies in the center of the frame and dominates most of the image.\n",
    "\n",
    "This is in stark contrast to satellite images.\n",
    "Here, all regions are equally _important_ and contribute to the overall scene.\n",
    "A popular alternative to ImageNet is the [Places](http://places2.csail.mit.edu/index.html) dataset {% cite Zhou2018 %}.\n",
    "The Places dataset focuses on training deep-learning models on scenes instead on specific objects.\n",
    "The main difference to remote images is the spatial resolution. While each pixel on the Places dataset covers a region of some square centimeters, every pixel from satellite imagery often covers double digit square meters! \n",
    "In other words, some remotely-sensed regions may have very few pixels associated with them, even if they are reasonably large and of high-interest.\n",
    "As a result of the high spatial resolution, _smaller_ classes {% fn 1 %} such as airports, port areas, or burnt forests, are severly underrepresented in datasets, while areas such as forests and water bodies dominate the data distribution.\n",
    "Also, each image can contain various scenes/objects.\n",
    "\n",
    "Another interesting difference, is the rotation invariance of such data.\n",
    "Due to the bird-eye view, there is no _top_ or _bottom_ for remote images{% fn 2 %}.\n",
    "\n",
    "|                                  |                  Classic RGB images                  |        Sentinel-2 multispectral images        |\n",
    "|----------------------------------|:----------------------------------------------------:|:---------------------------------------------:|\n",
    "| Spatial resolution of each pixel |                  from mm² up to cm²                  |                     ~XX m²                    |\n",
    "| Class distribution               |                       balanced                       |               heavily imbalanced              |\n",
    "| Rotation property                | Variant to rotation (there is a correct orientation) | Invariant (there is no \"correct\" orientation) |\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-c3ow\">Classic RGB images</th>\n",
    "    <th class=\"tg-c3ow\">Sentinel-2 multispectral images</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Spatial resolution of each pixel</td>\n",
    "    <td class=\"tg-c3ow\">from mm² up to cm²</td>\n",
    "    <td class=\"tg-c3ow\">~XX m²</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Class distribution</td>\n",
    "    <td class=\"tg-c3ow\">balanced</td>\n",
    "    <td class=\"tg-c3ow\">heavily imbalanced</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Rotation property</td>\n",
    "    <td class=\"tg-c3ow\">Variant to rotation (there is a correct orientation)</td>\n",
    "    <td class=\"tg-c3ow\">Invariant (there is no \"correct\" orientation)</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "- Show how hard it is to create such images\n",
    "- Ambiguity\n",
    "\n",
    "We understand how the data is represented and what the values stand for.\n",
    "But how do we work with these images?\n",
    "As most images use the RGB representation by default, there is a plethora of libraries and tools we could use to\n",
    "load, visualize and transform the images. \n",
    "As an example, we could use the `PIL` library we have seen in the previous introductory posts about [single](https://kai-tub.github.io/master-thesis-blog/images/2020/09/02/introduction-grayscale-images.html) and [multi-channel](https://kai-tub.github.io/master-thesis-blog/images/2020/09/16/images-with-channels.html) images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "img_path = Path(\".\") / \"2020-11-11\" / \"puppy.jpg\"\n",
    "opened_img = Image.open(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "        <div>\n",
    "            <figure id=\"Fig1\">\n",
    "<img src=\"2020-11-11/puppy.jpg\" alt=\"Image of a puppy\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig 1: Example RGB image (Image by <a href=\"https://pixabay.com/users/3194556-3194556/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1903313\">Karen Warfel</a> from <a href=\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1903313\">Pixabay</a>)</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "def center_crop(img, crop_size=512):\n",
    "    width, height = img.size\n",
    "    half_crop = crop_size // 2\n",
    "    # PILs coordinate system defines the pixel 0, 0 \n",
    "    # as the upper left corner of an image\n",
    "    left = width // 2 - half_crop\n",
    "    right = width // 2 + half_crop\n",
    "    upper = height // 2 - half_crop\n",
    "    lower = height // 2 + half_crop\n",
    "    return img.crop((left, upper, right, lower))\n",
    "    \n",
    "\n",
    "img = Image.open(img_path)\n",
    "cropped_rot_img = center_crop(img).rotate(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "cropped_rot_img.save(img_path.parent / \"cropped_rot_puppy.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "        <div>\n",
    "            <figure id=\"Fig2\">\n",
    "<img src=\"2020-11-11/cropped_rot_puppy.jpg\" alt=\"Center cropped and rotated image of the previous puppy\">\n",
    "            </figure>\n",
    "        </div>\n",
    "    <figcaption><center>Fig 2: Example transformation on RGB image (Image by <a href=\"https://pixabay.com/users/3194556-3194556/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1903313\">Karen Warfel</a> from <a href=\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1903313\">Pixabay</a>)</center></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more than 3 channels, only few libraries are avaible. The main question we have to ask ourselves is, what we want to do with these images? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "{% bibliography --cited_in_order %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{ 'In the deep-learning field, classes refer to the objects/areas of interest. We hope to train the model in such a way that it can differenciate between them. We will go into more detail in the next post.' | fndetail: 1 }}\n",
    "\n",
    "{{ 'This may seem obvious and irrelevant but it will play a bigger role in later posts, I promise 😉' | fndetail: 2 }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My idea\n",
    "My idea is to utilize the rotation invariance of the data.\n",
    "RotNet is based on the rotation _variance_. In other words, it is based on the fact that the objects/scenes had an *up* and *down*. The rotation is the label that should be predicted by the model. To solve this pretext-task the model should learn to identify the objects themselves, otherwise it wouldn't know the correct orientation.\n",
    "\n",
    "Due to the invariance of satellite data, this approach should not work.\n",
    "But what if we exploit the invariance of satellite data?\n",
    "If we rotate the image, the same/similar feature vector should be obtained.\n",
    "So what if use a siamese network with two input images and let the network try to identify the rotated original image?\n",
    "Or something similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myfastai] *",
   "language": "python",
   "name": "conda-env-myfastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
